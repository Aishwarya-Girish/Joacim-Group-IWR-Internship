[
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "This work would not have been possible without the collective support, guidance, and inspiration of numerous individuals.\nThank you Dr. Joacim Rocklöv for proving me the supportive research environment and resources through your wonderful group at the Interdisciplinary Centre for Scientific Computing (IWR), UNiversity of Heidelberg. I extend my sincere thanks to Dr. Marina Treskova for her mentorship, insightful critiques, and steady guidance, which greatly enhanced the quality and focus of this work.\nI am grateful to Dr. Hans-Peter Grossart for the invaluable opportunity to conduct research at the Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB). This experience was enriched by Dr. Nafi´u Abdulkadir, whose invitation and support facilitated a fruitful collaboration. I extended my warm gratitude to Sai Hariharan, Kushi, and Dr. Jaffer Yousouf Dar for their collegiality, knowledge sharing, and for providing samples for a smallparallel project. The time spent at IGB was not only highly educational but also truly memorable.\nLast, but certainly not the least, I reserve a special and heartfelt acknowledgment to Luise Nottmeyer. Her role extended far beyond timely mentorship. The countless brainstorming sessions and open, scientific discussions were instrumental in shaping the project’s trajectory and methodological rigor. Her cheerful spirit, wonderfully positive attitude, and great companionship made this internship a genuine pleasure!\nAuthor Details:\n\nName: Aishwarya Girish\nEmail: aishwarya.girish@stud.uni-heidelberg.de\nLinkedIn: https://www.linkedin.com/in/aishwaryagirish/"
  },
  {
    "objectID": "igb.html",
    "href": "igb.html",
    "title": "Research visit to the Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB), Stechlin-Neuglobsow",
    "section": "",
    "text": "Lake Stechlin, Neuglobsow (21/11/2024)"
  },
  {
    "objectID": "igb.html#prior-experiment-setup",
    "href": "igb.html#prior-experiment-setup",
    "title": "Research visit to the Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB), Stechlin-Neuglobsow",
    "section": "2.1 Prior Experiment Setup",
    "text": "2.1 Prior Experiment Setup\nThe study utilized four specialized floating structures in each experimental system. Each structure held \\(45\\) replicates of three substrate types: polyethylene terephthalate (PET), low-density polyethylene (LDPE), and glass, which served as a non-plastic control. The glass slides (\\(75\\text{ mm} \\text{x} 25\\text{ mm}\\)) were chosen as a standard proxy for mineral surfaces in biofilm research. Both PET and LDPE plastics were sourced from common household items and cut to the exact dimensions of the glass slides. All substrates were attached to the floating devices with binder clips. These structures were secured in the littoral zone (shoreline), keeping the substrates submerged about \\(10\\text{ cm}\\) below the surface and positioned to ensure they were not shaded by local plant life."
  },
  {
    "objectID": "igb.html#reagent-formulations",
    "href": "igb.html#reagent-formulations",
    "title": "Research visit to the Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB), Stechlin-Neuglobsow",
    "section": "2.2 Reagent Formulations",
    "text": "2.2 Reagent Formulations\nThe following reagents were prepared using DEPC-treated water:\n\n\\(2\\times\\) CTAB-Extraction Buffer (pH 8.0): \\(2\\%\\) CTAB (1 g), \\(1.4\\text{ M}\\) NaCl (4.09 g), \\(20\\text{ mM}\\) \\(\\text{Na}_2\\text{EDTA}\\) (0.29225 g), and \\(100\\text{ mM}\\) Tris-HCl (0.788 g), adjusted to \\(50\\text{ ml}\\).\nPEG/NaCl Solution: \\(1.6\\text{ M}\\) NaCl (4.7 g) and \\(30\\%\\) PEG 6000, adjusted to \\(50\\text{ ml}\\). Gentle heating (max. \\(50^\\circ\\text{C}\\)) was used to aid dissolution.\n\\(10\\%\\) SDS: \\(1\\text{ g}\\) in \\(10\\text{ ml}\\).\n\\(10\\%\\) N-Lauroylsarcosine: \\(1\\text{ g}\\) in \\(10\\text{ ml}\\).\nLinear Polyacrylamide (LPA): Used as a neutral carrier (Gen – Elute pro, \\(56575\\text{ Sigma}\\))."
  },
  {
    "objectID": "igb.html#metagenomic-dna-extraction",
    "href": "igb.html#metagenomic-dna-extraction",
    "title": "Research visit to the Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB), Stechlin-Neuglobsow",
    "section": "2.3 Metagenomic DNA Extraction",
    "text": "2.3 Metagenomic DNA Extraction\nPET, PE and Glass samples from the filter were processed for simultaneous DNA and RNA extraction using a modified CTAB/SDS/Phenol-Chloroform method (Wang et al., 2012; Brossier et al., 2004; Neresessian, 2005). The sample was transferred to a \\(2\\text{ ml}\\) screw-cap tube with zirconium beads (\\(0.1\\text{ mm}\\), \\(0.7\\text{ mm}\\), and \\(1\\text{ mm}\\)), followed by the addition of a lysis cocktail containing \\(650\\text{ }\\mu\\text{l}\\) of \\(2\\times\\) CTAB buffer, \\(65\\text{ }\\mu\\text{l}\\) of \\(10\\%\\) SDS, \\(65\\text{ }\\mu\\text{l}\\) of \\(10\\%\\) N-Lauroylsarcosine, and \\(20\\text{ }\\mu\\text{l}\\) of \\(20\\text{ mg/ml}\\) Proteinase K. The mixture was incubated at \\(60^\\circ\\text{C}\\) for \\(1\\text{ h}\\) without shaking. Mechanical cell disruption was achieved by adding \\(500\\text{ }\\mu\\text{l}\\) of Phenol-Chloroform-Isoamylalcohol (\\(\\text{25}:24:1\\)) and processing the tubes in a Fast Prep machine (\\(40\\text{ s}\\) at \\(6.0\\text{ m/s}\\)). Phases were separated by centrifugation (\\(10\\text{ min}\\) at \\(14,000\\times g\\) and \\(4^\\circ\\text{C}\\)), and the aqueous phase was transferred to a new tube. Residual phenol was removed by washing the aqueous phase with \\(650\\text{ }\\mu\\text{l}\\) of Chloroform-Isoamylalcohol (\\(\\text{24}:1\\)) and centrifuging again (\\(10\\text{ min}\\) at \\(14,000\\times g\\) and \\(4^\\circ\\text{C}\\)). The final aqueous phase was transferred to a new tube, and nucleic acids were precipitated by adding \\(2\\text{ volumes}\\) of PEG/NaCl solution and \\(2\\text{ }\\mu\\text{l}\\) of Linear Polyacrylamide (LPA) carrier, followed by an overnight incubation at \\(4^\\circ\\text{C}\\). The pellet was collected by centrifugation (\\(60\\text{ min}\\) at \\(17,000\\times g\\) and \\(4^\\circ\\text{C}\\)), washed with \\(800\\text{ }\\mu\\text{l}\\) of ice-cold \\(70\\%\\) ethanol (\\(10\\text{ min}\\) at \\(17,000\\times g\\) and \\(4^\\circ\\text{C}\\)), air-dried, and eluted in \\(50\\text{ }\\mu\\text{l}\\) of PCR-grade water after incubation (\\(10\\text{ min}\\) at \\(37^\\circ\\text{C}\\) at \\(400\\text{ rpm}\\))."
  },
  {
    "objectID": "igb.html#qubit-readings-ngml---15th-day",
    "href": "igb.html#qubit-readings-ngml---15th-day",
    "title": "Research visit to the Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB), Stechlin-Neuglobsow",
    "section": "3.1 QUBIT Readings (ng/ml) - 15th Day",
    "text": "3.1 QUBIT Readings (ng/ml) - 15th Day\n\n\n\nSample\nReading (ng/ml)\n\n\n\n\nPET-15-1\n44\n\n\nPET-15-2\n39\n\n\nPE-15-1\n7.8\n\n\nPE-15-2\n6.8\n\n\nGLASS-15-1\n39\n\n\nGLASS-15-2\n24\n\n\nW1-15-1\n78\n\n\nW1-15-2\n58\n\n\nW2-15-1\n59\n\n\nW2-15-2\n19\n\n\nW3-15-1\n0.114 / 1.60 / 1.34\n\n\nW3-15-2\n64\n\n\nBLANK-1\n0.0086 / 0.086\n\n\nBLANK-2\nLTB"
  },
  {
    "objectID": "igb.html#qubit-readings-ngml---30th-day",
    "href": "igb.html#qubit-readings-ngml---30th-day",
    "title": "Research visit to the Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB), Stechlin-Neuglobsow",
    "section": "3.2 QUBIT Readings (ng/ml) - 30th Day",
    "text": "3.2 QUBIT Readings (ng/ml) - 30th Day\n\n\n\nSample\nReading (ng/ml)\n\n\n\n\nPET-30-1\n35\n\n\nPET-30-2\n60\n\n\nPE-30-1\n50\n\n\nPE-30-2\n59\n\n\nGLASS-30-1\n91\n\n\nGLASS-30-2\n88\n\n\nW-30-1\n75\n\n\nW-30-2\n81\n\n\nW-30-3\n108\n\n\nBLANK-1\nLTB\n\n\nBLANK-2\nLTB\n\n\n\nNote: LTB = Lower than Blank\nQubit fluorometry showed a marked increase in DNA yield from Day 15 to Day 30 across all substrates. Early samples displayed low and variable concentrations, particularly in PE and W3 replicates, consistent with limited microbial colonization. By Day 30, DNA concentrations rose substantially, reaching up to \\(108\\text{ng/ml}\\), indicating active biofilm development and biomass accumulation. Blank controls remained below detection, confirming that the increase reflected genuine biological growth.\nSamples were then sent for metagenomic sequencing to profile the microbial communities."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "The processed data and analysis scripts are available on GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "",
    "text": "This internship report presents the methodological foundation and initial progress of an Artificial Intelligence Aided Systematic Literature Review investigating the role of microplastics in the propagation of antimicrobial resistance (AMR). The work detailed in the following sections represents the completed and ongoing efforts to construct a comprehensive and transparent review.\nWhile the final synthesis and manuscript preparation for the associated peer-reviewed publication are forthcoming, this document offers a complete overview of the established methodology so far, the implemented data processing pipeline, and the preliminary insights that will pave the way for the review’s completion."
  },
  {
    "objectID": "index.html#background-the-confluence-of-two-global-crises",
    "href": "index.html#background-the-confluence-of-two-global-crises",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "2.1 Background: The Confluence of Two Global Crises",
    "text": "2.1 Background: The Confluence of Two Global Crises\nThe Anthropocene is characterized by human-driven global changes that are reshaping planetary systems. Among the most pervasive and concerning of these are the dual crises of microplastic pollution [1], [2], [3] and antimicrobial resistance (AMR) [4], [5], [6], [7]. Each represents a formidable threat to ecosystem stability and public health, but their intersection creates a novel and synergistic challenge. Understanding the mechanisms by which these two crises converge is of paramount strategic importance for developing effective environmental and health policies for the 21st century.\nMicroplastic (MP) pollution has become a defining feature of our time, leading researchers to dub this era the “Plastic age.” MPs are synthetic polymer particles less than 5 mm in diameter, and their exceptional mobility and resistance to degradation have resulted in their near-ubiquitous presence in global ecosystems [8]. From freshwater rivers to deep-sea sediments, these particles act as persistent contaminants. The scale of the problem is staggering; projections estimate that global unmanaged plastic trash could reach 155–265 million metric tons per year in 2060 [9], [10], [11], [12]. This relentless accumulation of synthetic materials provides a novel and persistent substrate within natural environments, fundamentally altering microbial habitats on a planetary scale [13], [14]. Running parallel to the plastic crisis is the silent pandemic of Antimicrobial Resistance (AMR). Declared a “global public health issue” by the World Health Organization (WHO) and the Centers for Disease Control and Prevention (CDC), AMR threatens to undermine modern medicine [15], [16]. The overuse and misuse of antimicrobial drugs have accelerated the evolution of resistant microorganisms, rendering once-treatable infections deadly. The current toll is an estimated 700,000 annual deaths, a figure that is projected to escalate dramatically to 10 million by 2050 if left unchecked, surpassing the annual death toll from cancer.\nThese two crises converge on the surfaces of microplastics, where microbial communities colonize to form a distinct microecosystem known as the “plastisphere” [9], [17], [18], [19]. This biofilm-based habitat creates a unique ecological niche, with environmental conditions and microbial compositions that are markedly different from the surrounding water or natural particles. The plastisphere is not merely a passive carrier of microorganisms; it is a dynamic biological interface that can concentrate, protect, and foster interactions between diverse microbial taxa [20].\nThis review will explore the critical role of the plastisphere as a potential hotspot for the proliferation and global dissemination of antimicrobial resistance, a nexus of risk that demands urgent scientific inquiry."
  },
  {
    "objectID": "index.html#literature-context-the-plastisphere-as-a-vector-for-antimicrobial-resistance-genes-args",
    "href": "index.html#literature-context-the-plastisphere-as-a-vector-for-antimicrobial-resistance-genes-args",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "2.2 Literature Context: The Plastisphere as a Vector for Antimicrobial Resistance Genes (ARGs)",
    "text": "2.2 Literature Context: The Plastisphere as a Vector for Antimicrobial Resistance Genes (ARGs)\nThe plastisphere’s emergence as a novel ecological niche is of strategic importance not only for its role in altering biogeochemical cycles but also for its potential to accelerate the dissemination of antimicrobial resistance. Microplastics function as both incubators and transport vehicles for antibiotic resistance genes (ARGs) in aquatic environments [21].\n\n\n\nFigure 1. Microplastic-mediated transfer of antimicrobial resistance genes from plastisphere leading to human exposure.\n\n\nThe primary mechanism by which MPs facilitate the spread of ARGs is through the creation of a high-density microbial habitat. The biofilms that constitute the plastisphere are dense, spatially structured communities where bacteria are in close physical proximity [22]. These conditions are ideal for the exchange of genetic material through Horizontal Gene Transfer (HGT), a process that includes mechanisms such as plasmid exchange and transduction [23]. This turns each particle of microplastic into a potential hub for genetic exchange. Consequently, MPs function as both a “Petri dish” for the proliferation and selection of ARGs and as a “Vehicle” for their long-distance environmental dissemination [11], [18], [20], [21], [24]. Research employs advanced techniques like metagenomic sequencing, FTIR, and Scanning Electron Microscopy (SEM) to characterize microbial communities, ARGs, and the physical properties of MPs [25], [26].\nMounting evidence demonstrates a significant enrichment of ARGs within the plastisphere compared to surrounding environments. Studies conducted in diverse aquatic ecosystems, including the North Pacific and the Huangpu River, have found that MPs harbor a higher abundance of ARGs than adjacent water columns or natural particles [27], [28], [29]. The type of plastic polymer also appears to play a critical role in shaping the microbial community and its associated resistome [30]. Research indicates that different plastic types, such as biodegradable polylactic acid (PLA) and non-degradable polyethylene (PE), can selectively enrich distinct microbial communities and ARG profiles. Counterintuitively, some studies suggest that biodegradable MPs may pose a higher ARG risk, possibly because the degradation byproducts can serve as a carbon source, stimulating microbial growth and subsequent gene transfer [14], [20], [31], [32].\nWastewater systems are a critical focal point in this process. Wastewater treatment plants (WWTPs) are not only key conduits for the release of MPs into the environment but are also recognized hotspots for antibiotic-resistant bacteria (ARBs) and ARGs [33]. Although modern WWTPs can achieve a high removal efficiency for MPs (often exceeding 90%), they are not completely effective. The MPs that are discharged in treated effluent can carry a significant load of wastewater-associated pathogens and ARGs, effectively seeding natural ecosystems with a resilient, mobile source of antimicrobial resistance [17], [18], [21], [28]."
  },
  {
    "objectID": "index.html#research-gap-a-strategic-call-for-global-synthesis-to-unify-the-mp-amr-consensus",
    "href": "index.html#research-gap-a-strategic-call-for-global-synthesis-to-unify-the-mp-amr-consensus",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "2.3 Research Gap: A Strategic Call for Global Synthesis to Unify the MP-AMR Consensus",
    "text": "2.3 Research Gap: A Strategic Call for Global Synthesis to Unify the MP-AMR Consensus\nTo effectively assess and mitigate the public health risks posed by the MP-AMR intersection, a coherent, large-scale understanding of the underlying ecological dynamics is essential. However, the current body of research is characterized by a fragmentation of findings and a lack of synthesis, creating a strategic urgency to consolidate this knowledge into a unified framework.\nThe central research gap is the absence of a comprehensive, evidence-based consensus on the specific role of microplastics in ARG dissemination. Existing studies, while valuable, often lack unified conclusions, particularly regarding the critical question of whether different MP types selectively promote the enrichment of specific ARGs [21]. The limited scope of individual investigations, which are typically confined to specific geographic locations, plastic types, or environmental conditions, prevents a broader exploration of the complex ecological relationships that govern the plastisphere resistome across diverse spatiotemporal scales. A holistic understanding remains elusive.\nThis challenge is compounded by the sheer volume and rapid expansion of the relevant scientific literature. Thousands of articles have been published on factors related to environmental pollution and microbial resistance, yet a clear overview of all preceding factors and interaction between factors is missing.\nConsequently, the manual screening of titles and abstracts has become a major bottleneck for researchers [29]. The scale of the data is simply too vast for traditional review methodologies to handle effectively, leaving the field without a comprehensive map of the evidence.\nTo bridge this critical knowledge gap, a methodological solution is required that is both systematic in its approach and powerful enough to manage the immense scale of available data. This new strategy must move beyond isolated findings to build a structured, evidence-based understanding of the intertwined threats of microplastic pollution and antimicrobial resistance."
  },
  {
    "objectID": "index.html#approach-overview-a-systematic-ai-aided-literature-review",
    "href": "index.html#approach-overview-a-systematic-ai-aided-literature-review",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "2.4 Approach Overview: A Systematic, AI-Aided Literature Review",
    "text": "2.4 Approach Overview: A Systematic, AI-Aided Literature Review\nTo address the fragmented nature of the research landscape and overcome the bottleneck of manual screening, this study proposes a novel, AI-aided systematic literature review. This methodology is strategically designed to move beyond the analysis of individual findings and create a comprehensive, evidence-based map of the field, transforming disparate data points into a structured body of knowledge.\nA systematic review is the foundational component of this approach. This methodology is essential for synthesizing evidence from a large and varied body of scientific work [34], [35]. By systematically identifying, appraising, and synthesizing all relevant research on a specific topic, it allows for the creation of an “overview and strength of evidence” that is currently absent in the study of the plastisphere resistome. It provides a rigorous, reproducible framework for understanding the current state of knowledge and identifying areas for future research.\nThe integration of advanced Artificial Intelligence (AI) methodologies into the conduct of Systematic Reviews (SRs) is fundamentally driven by the need to manage the exponential growth in published literature and mitigate the inherent burdens associated with manual assessment, which often renders the process laborious, error-prone, and time-consuming and nearly impossible. To overcome this, an AI-powered active learning will be employed. This machine learning approach dramatically increases the efficiency of the screening process, with simulation studies showing it can “save up to 95% of screening time” [29]. The AI model in the software ASReview, operates in a “researcher-in-the-loop” process, where it learns from human decisions to continuously prioritize the most relevant records for review. This allows the research team to focus their expertise where it is most needed, ensuring both efficiency and accuracy [29], [36].\nThe proposed AI-aided workflow will involve a multi-phase screening process designed for both speed and precision. An initial screening phase will use a efficient model with sementic understanding to screen through the titles and abstracts and arrive at a consenses. This ensures that only few, if any, relevant papers are missed. A second phase will employ a more complex method of refining the selection of relevant articles and performing a full-text extraction, thus ensuring a high-quality final dataset.\nThe integration of systematic review rigor with artificial intelligence will enable this study to comprehensively map the research landscape. This powerful, evidence-based methodology will achieve specific, high-impact objectives that provide immediate and foundational value to the scientific and policy-making communities."
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "2.5 Objectives",
    "text": "2.5 Objectives\nThis research is guided by a dual set of objectives designed not only to answer a critical scientific question but also to create a lasting and valuable resource for the global research community.\nPrimary Objective: To synthesize the available evidence regarding the microplastic-AMR nexus through a comprehensive, AI-aided systematic review. This synthesis will specifically identify and quantify how a wide range of ecological and physico-chemical factors, including but not limited to plastic polymer type, environmental setting (e.g., freshwater, marine, wastewater effluent), and microbial community composition, influence the enrichment and spread of ARGs.\nThe secondary objective: To create a rich, queryable, and publicly available database of the screened literature. This database, containing hundereds of relevant articles identified through the AI-aided screening process, will serve as a basis for international researchers to conduct further, more specific meta-analyses and targeted investigations. This resource will function as a “living review”, designed to be updated over time as new research is published, thereby providing a dynamic and continuously evolving foundation for the field.\nThis review is conducted as a key activity within the scope of the EU-funded Project TULIP (Community-based engagement and intervenTions to stem the tide of antimicrobial resistance spread in the aqUatic environments catalysed by cLImate change and Plastic pollution interactions). TULIP represents a crucial international effort to address the triple threat of microplastic pollution, antimicrobial resistance (AMR), and climate change in aquatic environments."
  },
  {
    "objectID": "index.html#protocol-development",
    "href": "index.html#protocol-development",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "3.1 Protocol Development",
    "text": "3.1 Protocol Development\nThis study was preregistered on the Open Science Framework (OSF) prior to data collection and analysis. The registration was completed on 18th February 2025 and outlines the primary research questions, methodological approach, and analytical strategy.\nThe preregistration specified the following key elements:\n\nSearch Strategy\nScreening Procedure\nData Extraction and Synthesis\nResearch Objectives\n\nThis was done so that any deviations from the original registration could be noted in the respective methodology and results sections of the review. This implies that exploratory analyses conducted beyond the registered plan could be clearly identified to maintain transparency between confirmatory and exploratory findings.\nThe complete registration document, including the original timestamped research plan, is publicly available at https://osf.io/23wnh/overview, ensuring the reproducibility and verification of our research process."
  },
  {
    "objectID": "index.html#query-design",
    "href": "index.html#query-design",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "3.2 Query Design",
    "text": "3.2 Query Design\nThe search strategy was systematically developed and validated to ensure high sensitivity in capturing the literature at the intersection of plastic pollution and antimicrobial resistance.\nBibliographic Databases and Search String: Comprehensive electronic searches were performed across four major bibliographic databases: PubMed, Embase, Web of Science, and GreenFile. The search strategy employed a Boolean query designed to capture two core concepts:\nAntimicrobial Resistance (AMR): Terms included “antimicrobial resistance”, ”antibiotic resistance”, “drug resistance”, AMR, ”resistance gene”, and “resistome*“.\nPlastic Pollution: Terms included plastic, plastics, microplastic, nanoplastic, and plastisphere.\nThe final query used was:\n(\"antimicrobial resistance*\" OR \"antibiotic resistance*\" OR \"antimicrobial-resistant\" OR \"antibiotic-resistant\" OR \"drug resistance*\" OR \"multidrug resistance*\" OR amr OR \"resistance gene*\" OR arg OR ARGs OR \"resistant bacterium\" OR \"resistant bacteria\" OR resistome*)\nAND\n(plastic OR plastics OR microplastic* OR nanoplastic* OR plastisphere)\nThe search was limited to peer-reviewed articles published in English between 1 January 2015 and 10 February 2025.\nSearch Validation Procedure:\nTo ensure the robustness and comprehensiveness of the search strategy, a formal validation was conducted using a set of benchmark articles. Sixteen key publications were pre-identified as representative of the diverse study types and themes within the field (e.g., laboratory studies, field observations, and reviews). Multiple iterations of the search syntax were tested against these articles. The final query was selected because it successfully retrieved all 16 validation articles, confirming its ability to capture a wide spectrum of relevant literature. This process validates that the search strategy possesses the necessary sensitivity to serve as a reliable foundation for the systematic mapping."
  },
  {
    "objectID": "index.html#automated-pubmed-data-extraction-pipeline",
    "href": "index.html#automated-pubmed-data-extraction-pipeline",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "3.3 Automated PubMed Data Extraction Pipeline",
    "text": "3.3 Automated PubMed Data Extraction Pipeline\n\n\n\nFigure 2. Automated PubMed data extraction pipeline.\n\n\nA central component of this project was the development of a Python-based data extraction pipeline that automates the retrieval, parsing, and organization of literature from the PubMed database. The overarching aim of this system is to establish a foundation for AI-aided literature review, allowing for large-scale, reproducible aggregation of scientific articles.\nBy automating the collection of metadata such as titles, abstracts, authorship, and publication details, the system enables downstream analysis of the retreived data. This not only accelerates information gathering but also ensures methodological transparency, as every step (from querying to parsing) is reproducible.\n\n3.3.1 Library Setup and Core Architecture\nThe implementation makes use of the requests, lxml, and BeautifulSoup [37], [38] libraries to interact with the PubMed Application programming interface (API) and process its XML-based responses. The E-utilities are the public API to the NCBI Entrez system and allow access to all Entrez databases including PubMed, PMC, Gene, Nuccore and Protein. The NCBI E-utilities endpoint serves as the interface for fetching article metadata [39], [40]. Each component of the pipeline plays a distinct role: requests manages HTTP communication, lxml performs efficient XML tree parsing, and BeautifulSoup ensures robust cleaning of textual data embedded in HTML or XML tags.\nimport requests\nfrom lxml import etree\nfrom bs4 import BeautifulSoup\nimport csv, os\n\nbase_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\nHere, the variable base_url defines the central API endpoint from which two specific utilities esearch.fcgi and efetch.fcgi are subsequently called. These utilities respectively perform article identification and detailed metadata retrieval [41].\n\n\n3.3.2 Data Sanitization and Preprocessing\nData fetched from PubMed often contain embedded markup elements, which complicate downstream text processing. To address this, a dedicated cleaning function was implemented using the BeautifulSoup HTML parser. This function ensures that the text is free of structural tags and extraneous characters, producing plain text suitable for machine learning models and vector-based text representations.\ndef clean_html(raw_html):\n    soup = BeautifulSoup(raw_html, \"html.parser\")\n    return soup.get_text()\nThis step is critical for maintaining the linguistic integrity of the abstracts and titles while preventing formatting artifacts from interfering with tokenization or semantic embedding stages in later analysis. It also ensures that multilingual and symbol-rich abstracts are handled consistently.\n\n\n3.3.3 Querying and Fetching Data from PubMed\nThe heart of the retrieval system lies in the fetch_all_pubmed_data() function, which governs the two-stage data acquisition process. In the first stage, the function submits an E-Search request to identify all publication identifiers (PMIDs) associated with the user’s query, optionally constrained by a publication date range.\nThe function then performs iterative requests in batches, ensuring scalability and compliance with PubMed’s API rate limits. In the second stage, the E-Fetch utility is invoked to retrieve detailed article metadata corresponding to each batch of PMIDs.\nThe XML response is parsed using lxml, allowing structured extraction of key bibliographic fields. The fields extracted were:\n\n\n\nTable 1. Relevant data fields extracted from PubMed.\n\n\nDuring extraction, HTML/XML artifacts embedded in titles and abstracts were removed to ensure consistency in textual data. The resulting output is stored as a structured list of dictionaries and subsequently converted into a dataframe for downstream analysis. This approach ensures high fidelity retrieval while allowing flexible modification of query scope and record fields.\nThe implementation deliberately employs batch processing (with a default of 200 records per request) to efficiently handle large datasets while avoiding excessive memory usage. Furthermore, optional date filtering provides temporal control over the dataset, facilitating focused meta-analyses over defined publication windows.\nThe full implementation used in this study is available at: https://github.com/Aishwarya-Girish/Joacim-Group-IWR-Internship/blob/main/1-Article_Data/pubmed.py\nExample Usage and Illustrative Simplified Code:\nimport requests\nfrom lxml import etree\n\ndef fetch_all_pubmed_data(query, start_date=None, end_date=None):\n    \"\"\"\n    Simplified illustrative version:\n    Fetches PMIDs based on a query and retrieves core article metadata.\n    \"\"\"\n    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n\n    # Optional date filter\n    if start_date and end_date:\n        query += f' AND ({start_date}[PDAT] : {end_date}[PDAT])'\n\n    # --- Step 1: Retrieve PMIDs ---\n    pmid_response = requests.get(\n        base_url + \"esearch.fcgi\",\n        params={\n            \"db\": \"pubmed\",\n            \"term\": query,\n            \"retmax\": 200000,\n            \"retmode\": \"xml\",\n            \"sort\": \"relevance\"\n        }\n    )\n    pmid_tree = etree.fromstring(pmid_response.content)\n    pmids = [elem.text for elem in pmid_tree.findall(\".//Id\")]\n\n    results = []\n\n    # --- Step 2: Retrieve details for PMIDs in batches ---\n    for i in range(0, len(pmids), 200):\n        batch = pmids[i:i+200]\n        detail_response = requests.get(\n            base_url + \"efetch.fcgi\",\n            params={\"db\": \"pubmed\", \"id\": \",\".join(batch), \"retmode\": \"xml\"}\n        )\n        detail_tree = etree.fromstring(detail_response.content)\n\n        for article in detail_tree.findall(\".//PubmedArticle\"):\n            title = article.findtext(\".//ArticleTitle\", default=\"No title available\")\n            abstract = article.findtext(\".//AbstractText\", default=\"No abstract available\")\n            year = article.findtext(\".//PubDate/Year\", default=\"Unknown year\")\n\n            results.append({\n                \"PMID\": article.findtext(\".//PMID\", default=\"\"),\n                \"Title\": title.strip(),\n                \"Abstract\": abstract.strip(),\n                \"Year\": year\n            })\n\n    return results\n\n# Example call\npapers = fetch_all_pubmed_data(\"single-cell RNA sequencing\", start_date=\"2015\", end_date=\"2024\")\n\n\n3.3.4 Data Structuring and Storage\nOnce the data retrieval is complete, the results are serialized and exported into a structured CSV format. The function below ensures that the output directory is automatically created if absent and that the dataset is encoded in UTF-8 to preserve non-ASCII characters in author names or abstracts.\ndef save_to_csv(data, filename=\"pubmed.csv\"):\n    if not os.path.exists(\"Data\"):\n        os.makedirs(\"Data\")\n    with open(os.path.join(\"Data\", filename), \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n        writer = csv.DictWriter(f, fieldnames=data[0].keys())\n        writer.writeheader()\n        writer.writerows(data)\nThis step transforms the raw API output into a machine-readable corpus suitable for AI-driven literature analysis, such as embedding-based similarity searches, topic modeling, and trend visualization. Each record corresponds to a distinct publication, with fields standardized across all entries.\n\n\n3.3.5 Integration and Execution\nThe complete system is integrated through a main function that reads user-defined queries from a text file and executes the retrieval and storage workflow.\nThis modular design ensures reproducibility and adaptability. The system can be operated as a standalone command-line tool or integrated as a backend service within larger text-mining workflows. The resulting dataset can subsequently be fed into AI-assisted frameworks."
  },
  {
    "objectID": "index.html#hybrid-data-retrieval-strategy",
    "href": "index.html#hybrid-data-retrieval-strategy",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "3.4 Hybrid Data Retrieval Strategy",
    "text": "3.4 Hybrid Data Retrieval Strategy\nIn order to complement the automated PubMed pipeline, additional searches were conducted across EMBASE, GreenFILE, and Web of Science (WoS) to achieve a broader literature coverage. The query used was the same as was previously used for PubMed.\nUnlike PubMed, these databases have restricted access to their APIs. EMBASE and Web of Science, in particular, require logins, institutional subscriptions or university credentials to access their APIs, advanced search features and bulk export functionalities, while GreenFILE similarly limits automated access. Consequently, it was necessary to perform manual data extraction through the web interfaces of each database.\nFor the databases Web of Science, EMBASE and GreenFILE, all available metadata fields along with the abstract were exported directly from the database interface. This ensured that no potentially relevant semantic or contextual information was lost prior to downstream computational analysis, which is particularly important for AI-driven literature synthesis that benefits from the full depth of structured metadata.\nBy exporting the entire metadata schema from each database, the resulting dataset remained maximally flexible and analytically complete. This approach avoids the methodological bias that can arise from selective field extraction.\nThis manual extraction process, while complete in terms of metadata retention, highlighted the practical limitations of restricted bibliographic databases when compared to the PubMed automated pipeline. Unlike PubMed, where programmatic retrieval was efficient, reproducible, and less susceptible to human transcription or export errors, the workflows for EMBASE, GreenFILE, and Web of Science required repeated manual operations. This introduced additional time burdens and increased the potential for inconsistencies in formatting and data organization across exports. As a result, the contrast between the two approaches underscored the value of automated retrieval methods in terms of speed, error reduction, and standardization — demonstrating that, where possible, API-driven pipelines provide a more robust and scalable foundation for downstream computational analysis.**"
  },
  {
    "objectID": "index.html#standardization-of-data",
    "href": "index.html#standardization-of-data",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "3.5 Standardization of Data",
    "text": "3.5 Standardization of Data\nTo facilitate the standardization of heterogeneous datasets from diverse platforms, a structured, machine-readable corpus suitable for the AI-aided literature review was created. By integrating these manually curated datasets with the automated PubMed collection, the framework supports large-scale text mining, cross-database analyses, and knowledge synthesis in a reproducible and systematic manner.\n\n3.5.1 Metadata Standardization and Column Alignment\nBecause each database uses distinct terminology and variable naming conventions, key metadata fields (Title, Abstract, Authors, DOI, Journal) were standardized to a shared schema.\npubmed_df_new = pubmed_df[['Title', 'Abstract', 'Authors', 'DOI', 'Journal']]\nwos_df_new = wos_df[['Article Title', 'Abstract', 'Authors', 'DOI', 'Source Title']].rename(\n    columns={'Article Title': 'Title', 'Source Title': 'Journal'})\nThis harmonization step allowed all datasets to be concatenated into a unified dataframe (compiled_df) suitable for cross-database comparison.\n\n\n3.5.2 Text Normalization for Duplicate Detection\nSome records lacked DOIs, which made conventional duplicate detection insufficient. To enable robust identification of duplication based on article content, text fields were normalized by removing punctuation and enforcing consistent case formatting:\ndef normalize_text(text):\n    return ''.join([char.lower() for char in text if char not in string.punctuation])\nThis normalization was applied to titles, abstracts, authors, and journal names to allow structural comparison even where metadata formatting differed.\n\n\n3.5.3 Identification of Duplicates and Statistical Summary\nDuplicates were first detected using DOIs, where available, since DOI values are globally unique identifiers. For records without DOIs, duplicates were inferred based on normalized textual similarity across key descriptive fields:\nreplicate_counts = df['DOI'].value_counts()\ndf_no_doi['Normalized_Title'] = df_no_doi['Title'].apply(normalize_text)\nThe ouput of the script consisted of:\n\nThe number of unique articles.\nThe number of replicated entries.\nThe number and proportion of records lacking DOIs.\nReplication counts among no-DOI entries across Title, Abstract, Authors, and Journal fields.\n\nThese results were exported as .csv tables and a summary .txt file for documentation and quality control.\n\n\n\nFigure 3. Results of database compilation and dedepulication."
  },
  {
    "objectID": "index.html#automated-bibliographic-metadata-retrieval",
    "href": "index.html#automated-bibliographic-metadata-retrieval",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "3.6 Automated Bibliographic Metadata Retrieval",
    "text": "3.6 Automated Bibliographic Metadata Retrieval\nLarge-scale literature reviews require the management of extensive citation libraries, where manual retrieval of reference metadata is both inefficient and prone to formatting inconsistencies. Unique identifiers such as DOIs provide a reliable key for automated access to standardized citation records, enabling reproducible and consistent bibliographic handling across heterogeneous data sources.\nTo operationalize this, a custom Python-based retrieval pipeline was implemented to programmatically obtain BibTeX entries for all articles in the dataset. The script accepts a CSV containing DOIs and queries the CrossRef REST API [42], [43] for each record, executing requests concurrently using a five-worker thread pool to reduce latency and improve throughput. This automated approach ensured uniform reference formatting, minimized manual intervention, and produced a structured citation corpus suitable for downstream analysis.\nEach DOI request was subject to a retry policy with exponential backoff to handle temporary network failures or rate limiting. Successful responses were stored individually as .bib files in a designated output directory, with filenames normalized to ensure compatibility with local file systems. DOIs that could not be resolved or returned invalid responses were recorded separately.\ndef fetch_bibtex(doi, max_retries=3, timeout=15):\n    api_url = f\"https://api.crossref.org/works/{doi}/transform/application/x-bibtex\"\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(api_url, timeout=timeout)\n            if response.status_code == 200:\n                return doi, response.text\n        except requests.exceptions.Timeout:\n            sleep(randint(1, 3))\n    return doi, None\nTo optimize performance when processing thousands of DOIs, the system employs concurrent programming using a thread pool. This allows multiple API requests to execute simultaneously, dramatically reducing total execution time compared to sequential processing.\ndef process_bibtex_entries(input_csv, output_folder, failed_csv, success_csv):\n    input_df = pd.read_csv(input_csv)\n    dois = input_df['DOI']\n    bibtex_results = {}\n\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        future_to_doi = {executor.submit(fetch_bibtex, doi): doi for doi in dois}\n        for future in as_completed(future_to_doi):\n            doi, bibtex_content = future.result()\n            bibtex_results[doi] = bibtex_content\nTo ensure reproducibility and transparency, four types of logs were generated:\n\nTable 2. Output structure of the BibTex Extraction code.\n\n\n\n\n\n\n\nOutput Artifact\nContents\nPurpose\n\n\n\n\nbibtex_files/*.bib\nIndividual BibTeX citation files\nInput to reference managers / LaTeX\n\n\nsaved_entries.csv\nDOIs successfully retrieved\nAccountability and success summary\n\n\nfailed_entries.csv\nDOIs that could not be fetched\nManual follow-up (e.g., publisher-restricted)\n\n\nbibtex_stats.txt\nAggregate retrieval statistics\nWorkflow record-keeping and reproducibility\n\n\n\nThis automated approach enabled high-throughput citation acquisition, ensured consistent formatting and minimized manual intervention.\nThe output could also be integrated with softwares such as Mendeley, Zotero and LaTeX."
  },
  {
    "objectID": "index.html#asreview-a-scalable-ai-augmented-framework-for-accelerating-systematic-literature-review",
    "href": "index.html#asreview-a-scalable-ai-augmented-framework-for-accelerating-systematic-literature-review",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "3.7 ASReview : A Scalable, AI-Augmented Framework for Accelerating Systematic Literature Review",
    "text": "3.7 ASReview : A Scalable, AI-Augmented Framework for Accelerating Systematic Literature Review\nASReview functions as an AI-aided Systematic Reviewing tool, specifically engineered as an open-source machine learning framework that aims to aid in efficient and transparent systematic reviews [29], [36].\n\n3.7.1 Server Setup and Sotware Overview\nA server-based instance of the open-source software framework ASReview was deployed at the Interdisciplinary Center for Scientific Computing (IWR) at Heidelberg University. This setup provides a high-performance, multi-user platform for the AI-aided screening of scientific literature.\nIts primary task is the semi-automation of the demanding literature screening phase (titles and abstracts), aiming to streamline the identification and selection of primary studies. This approach leverages an Active Learning (AL) pipeline, integrating a transformer-based language model for semantic feature extraction and a robust ensemble classifier for prioritization, demonstrating a principled methodology to drastically reduce the manual screening burden while preserving the rigorous recall demanded by systematic review standards.\n\n\n3.7.2 Methodological Framework\nThe ASReview pipeline is engineered around an iterative Active Learning (AL) cycle, designed to minimize the cardinality of the manually screened set required to identify nearly all relevant documents. Formally, given a corpus of \\(N\\) documents \\(D = \\\\{d_1, d_2, ..., d_N\\\\}\\), the AL algorithm iteratively selects a document \\(d^*\\) from the unlabeled pool \\(U_t\\) for labeling by a human reviewer (the ‘oracle’).\nThis selection is made by a query strategy, which acts as a ranking engine. After each human labeling action, the model reassesses the entire unlabeled pool, predicting the probability of relevance for every document. It then identifies and promotes the single most promising document (the one with the highest predicted relevance) to the top of the screening queue. This dynamic re-ranking ensures the reviewer is always presented with the next most likely relevant record, effectively rearranging the pile in real-time to maximize screening efficiency.\nThe working pipeline for this model is comprised of these five synergistic components that can be customized for the best desired output:\n\nFeature Extraction\nClassification Algorithm\nQuery Strategy\nBalancing Strategy\nStopping Rule\n\n\n\n3.7.3 Feature Extraction\nA feature extractor is a model that transforms raw data into a compact set of informative variables for downstream analysis or prediction. This function \\(\\phi: d_i \\rightarrow \\mathbf{x}_i \\in \\mathbb{R}^{768}\\) maps a document’s text to a dense vector representation.\nIn this case, the all-mpnet-base-v2 model was employed, a variant of Sentence-BERT (Bidirectional Encoder Representations from Transformers).\nThis model is based on the microsoft/mpnet-base architecture, a pre-trained Transformer network that uses masked and permuted language modeling objectives to develop a deep, contextual understanding of language. It was further fine-tuned on a massive dataset of over 1 billion sentence pairs using a contrastive learning objective. The training aim was to minimize the distance between embeddings of semantically similar sentences (e.g., paraphrases) while maximizing the distance between dissimilar ones, creating a semantically meaningful 768-dimensional latent space.\nUnlike dataset-specific methods like Doc2Vec, the dimensions of an SBERT embedding are not directly interpretable but are fixed, abstract features learned during this large-scale pre-training, capturing complex linguistic properties.\n\n\n3.7.4 Classification Algorithm\nA Random Forest (RF) classifier was utilized, an ensemble of \\(K\\) decision trees \\(\\\\{T_1, T_2, ..., T_K\\\\}\\). In a random forest classifier, each tree \\(T_k\\) is grown on a bootstrapped sample of the training data and a random subset of the \\(m\\) features, injecting robustness. The final relevance probability for a document with embedding \\(\\mathbf{x}_i\\) is given by:\n\\[\n\\hat{y}_i = \\frac{1}{K} \\sum_{k=1}^{K} T_k(\\mathbf{x}_i)\n\\]\nThis ensemble approach is particularly effective in the AL context. Its inherent stability mitigates overfitting to the small, imbalanced labeled sets \\(L_t\\) prevalent in early screening stages. Furthermore, its non-parametric nature allows it to capture complex, non-linear decision boundaries in the SBERT embedding space, making it exceptionally powerful for identifying the sparse, heterogeneous relevant records in the later stages of screening.\n\n\n3.7.5 Query Strategy\nThe system employs a maximum certainty query strategy, selecting for review the document with the highest predicted probability of relevance.\n\\[\nd^* = \\arg\\max_{d_i \\in U_t} \\hat{y}_i .\n\\]\nWhere: - \\(d^*\\) = the selected document for labeling - \\(U_t\\) = set of unlabeled records at time \\(t\\) - \\(\\hat{y}_i\\) = predicted relevance score for document \\(d_i\\)\nThis approach directly prioritizes the records most likely to be relevant.\n\n\n3.7.6 Balancing Strategy\nTo counteract the extreme class imbalance (typically &gt;95% irrelevant records), a dynamic balancing algorithm was used to ensure the training data for the RF contains a representative ratio of relevant and irrelevant instances, preventing classifier bias.\n\n\n3.7.7 Stopping Rule\nThe process can be terminated by a formal stopping rule, such as achieving a pre-specified Work Saved over Sampling (WSS) value, which quantifies the fraction of screening saved at a target recall level \\(R\\):\n\\[\n\\text{WSS}@R = 1 - \\frac{\\text{records screened to find } R \\times \\text{total relevants}}{N}\n\\]\nExample from ASReview output: - \\(\\text{WSS}@0.95 = 0.85\\): 85% work saved finding 95% of relevant studies - \\(\\text{WSS}@0.90 = 0.90\\): 90% work saved finding 90% of relevant studies\n\n\n3.7.8 Visualizing the Feature Space\nThe superiority of the SBERT-RF synergy is rooted in the quality of the feature space. A deeper understanding and good illustration for the same can be found here.\nIn their example, they project embeddings from three different feature extractors—TF-IDF, Doc2Vec, and SBERT—for a set of simple sentences onto two dimensions using Principal Component Analysis (PCA).\n\nTF-IDF & Doc2Vec: These methods typically produce clusters based on keyword overlap or shallow semantic similarity. For instance, “I love cookies” and “I hate cookies” may be placed close together due to the shared word “cookies,” failing to capture their opposing sentiments.\nSBERT: The transformer-based embeddings create a geometrically superior layout. Sentences are clustered by semantic meaning rather than just lexical overlap. “I love cookies” and “I love cake” form one cluster (positive sentiment about food), while “I hate cookies” and “I hate cake” form another (negative sentiment). The sentence “I like desserts” is appropriately positioned near the positive cluster. This nuanced representation provides the RF classifier with a highly discriminative space in which to learn an effective decision boundary for relevance.\n\nThus, by deploying a high-performance, server-based ASReview instance and leveraging the advanced representational capabilities of SBERT coupled with the stable, non-parametric learning of Random Forests, a scalable and scientifically rigorous solution to the systematic review bottleneck was provided.\n\n\n\nFigure 4. (a) Porject Overview interface of the ASReview Sodtware. (b) Project History page of ASReview"
  },
  {
    "objectID": "index.html#preliminary-data-extraction-using-chatgpt",
    "href": "index.html#preliminary-data-extraction-using-chatgpt",
    "title": "Accelerating Systematic Reviews: An AI-Aided Workflow for Synthesizing the Microplastic-Antimicrobial Resistance Knowledge Landscape",
    "section": "3.8 Preliminary Data Extraction using ChatGPT",
    "text": "3.8 Preliminary Data Extraction using ChatGPT\nAfter the relevant papers are retrieved from the ASReview screening, specific information such as source, plastic types, methods used, and other key information need to be extracted. Performing this task manually across hundreds of papers is both time-intensive and cognitively demanding. The goal of this code is to evolve into a general-purpose information scanning framework that can automatically interpret such fields and extract domain-specific information based on user-defined criteria by employing a large language model (LLM).\n\n3.8.1 General Setup\nThe implementation begins by setting up the Python environment using the pandas and openai libraries. Pandas handles data loading and CSV operations [44], while the OpenAI library [45] enables interaction with the GPT model through an authenticated API client. The script reads an input CSV file containing research paper titles and abstracts into a DataFrame and verifies the presence of both required columns. This ensures the dataset is properly structured for analysis. Finally, a results list is initialized to store each paper’s title, abstract, and extracted information identified by the model.\n\n\n3.8.2 Explanation of Iterating Through Rows and Fetching Data from the ChatGPT API\nThis code integrates ChatGPT (via the OpenAI API) to automatically extract structured information from research papers, using fields such as the title and abstract.\nWhile this basic example illustrated below focuses on identifying polymers, sample source and classifying papers as review or primary studies, the approach is general-purpose and can be adapted to extract any form of domain-specific information — such as methods, materials, results, or keywords — depending on the research goal.\nThe most critical component is the prompt, which defines the model’s role, task, and expected output format. A well-designed prompt ensures that ChatGPT interprets scientific text accurately and returns consistent, structured responses suitable for large-scale automated analysis.\nfor _, row in df.head(100).iterrows():\n\n\n   title = str(row[\"Title\"])\n   abstract = str(row[\"Abstract\"])\n  \n   text = f\"Title: {title}\\nAbstract: {abstract}\"\n\n\n   # Step 3: Prompt ChatGPT for plastics, paper type, and source type\n   prompt = f\"\"\"\n   You are an expert in environmental and materials science, specialized in analyzing research literature.\n\n\n   Read the following research paper title and abstract, and identify three pieces of information:\n\n\n   1. **Plastics or Polymers**: List all types of plastics or polymers mentioned.\n      - If none are mentioned, return 'None'.\n\n\n   2. **Paper Type**: Determine whether the paper is a 'Review Paper' or a 'Primary Study'.\n      - A review paper summarizes prior research.\n      - A primary study presents new experiments or data.\n\n\n   3. **Source Type**: Identify the environmental or sampling source being studied.\n      - Examples: River, Estuary, Bay, Lake, Reservoir, Mangrove, WWTP Effluent, Open Ocean, Marine, Intertidal Zone, etc.\n      - If unclear, return 'Unknown'.\n     \n   4.  **Method_AR_Detection**:Summarize the antibiotic resistance detection methodology\n       - Examples: qPCR, PCR, Metagenomics, sequencing platforms, reference databases, bioinformatics tools, etc.\n\n\n   Return your answer strictly in JSON format as:\n   {{\n       \"plastics_found\": \"...\",\n       \"paper_type\": \"...\",\n       \"source_type\": \"...\",\n       method_ar_detection: \"...\"\n   }}\n\n\n   Text:\n   {text}\n   \"\"\"\n\n\n   # Step 4: Query ChatGPT\n   response = client.chat.completions.create(\n       model=\"gpt-4o-mini\",\n       messages=[\n           {\"role\": \"system\", \"content\": \"You are a scientific text analysis assistant.\"},\n           {\"role\": \"user\", \"content\": prompt}\n       ]\n   )\n\n\n   # Step 5: Extract and parse response\n   content = response.choices[0].message.content.strip()\nResults:\n\nTable 3. Example output for the preliminary text extraction using ChatGPT.\n\n\n\n\n\n\n\n\n\nTitle\nAbstract\nPlastics Found\nPaper Type\nSource Type\n\n\n\n\nPlastic Polymers And Antibiotic Resist…\nMicrobial colonization of plastic…\nPVC, PE\nPrimary Study\nRoss Sea\n\n\nThe Variation Of Various Microplastics On…\nWastewater treatment plants (WWTP…\nPET, PE, PLA, PVC\nPrimary Study\nWWTP\n\n\nMarine Plastisphere Selectively Enriches…\nSeveral studies have focused o…\nPE, PP, PS, PVC\nPrimary Study\nMarine\n\n\nTaxonomic Variation, Plastic Degradation…\nWastewater treatment facilities…\nLDPE, nylon-6, PET, polylactic acid, oxygen\nPrimary Study\nWWTP\n\n\nBioplastic Accumulates Antibiotic And Me…\nThe oceans are increasingly pol…\npolyhydroxyalkanoate, polyethylene terephthalate (PET)\nPrimary Study\nMarine\n\n\nEffects Of Microplastics On Antibiotic Re…\nMicroplastics and antibiotic res…\nPE, PVC, PVA\nPrimary Study\nEstuary\n\n\nMicroplastics Shape Microbial Interactor…\nWastewater treatment plants (WW…\npolyethylene terephthalate, silicone resin\nPrimary Study\nWWTP\n\n\nPlastisphere Showing Unique Microbial…\nPlastisphere (the biofilm on micr…\nPVC\nPrimary Study\nWWTP\n\n\nSize Effects Of Microplastics On Antibiotic…\nMicroplastics (MPs) in aquatic…\npolyethylene, polypropylene, polystyrene\nPrimary Study\nRiver\n\n\nMetagenomic Analysis Reveals The Effects…\nSewage sludge is recognized as b…\nPE, PET, PLA\nPrimary Study\nWWTP\n\n\nMicroplastics Exacerbate The Ecological…\nWetlands are vital components o…\npolyethylene, polypropylene\nPrimary Study\nLake\n\n\nResponses Of Bacterial Communities And…\nIn present study, copper (Cu), Zn…\n-\nPrimary Study\nSewage\n\n\nAntibiotic Resistance Genes In Biofilms Of…\nPlastic wastes are ubiquitous i…\nPE, PP, PET\nPrimary Study\nEstuary\n\n\nDistribution And Potential Ecological Ris…\nThe interaction between microplast…\nPolyethylene, Silicone\nPrimary Study\nBay\n\n\nSource Antibiotics Degradation And High-Ris…\nMicroplastics (MPs) are increasin…\n-\nPrimary Study\nUrban aquatic ecosystems\n\n\nRiverine Microplastics And Bacterial Co…\nMicroplastics could serve as mat…\nPP, PET\nPrimary Study\nRiver\n\n\nMicroplastics Affect Bacterial Communiti…\nAlong with global plastic produc…\nPE, PP, PVC, PET\nPrimary Study\nEstuary\n\n\nCombined Environmental Pressure Induce…\nThe characteristics and dynamic poly…\npolylactic acid\nPrimary Study\nUnknown\n\n\nSize-Dependent Promotion Of Micro/Nano…\nConstructed wetlands (CWs) has poly…\nPS\nPrimary Study\nUnknown\n\n\nUnderstanding The Mechanism Of Micropl…\nThe pervasive presence of micropl…\nPHA, PLA\nPrimary Study\nUnknown\n\n\nSize-Dependent Effects Of Microplastic…\nMicroplastics (MPs) were combined, …\nPE, PET\nPrimary Study\nUnknown\n\n\nTetracycline Accumulation In Biofilms Enh…\nMicroorganisms are present in …\n-\nPrimary Study\nUnknown"
  },
  {
    "objectID": "future_research.html",
    "href": "future_research.html",
    "title": "Directions for Future Research",
    "section": "",
    "text": "This section outlines a forward-looking roadmap designed to build directly upon the robust methodological foundation established in this report. The future trajectory of this project will pivot from methodological construction to the generation of high-impact scientific insights and the creation of a lasting resource for the international research community.\nThe immediate next steps are to complete the systematic review and translate the curated data into publishable findings. This involves:\nComprehensive Full-Text Analysis:\nMoving beyond the completed title-and-abstract screening to a full-text review and detailed data extraction from all relevant articles. This will capture the fine-grained data required to fulfill the primary research objective and fuel the advanced synthesis below.\nAdvanced Data Synthesis and Modeling: The unified dataset will be analyzed using a multi-faceted approach to move from fragmented findings to a consolidated understanding. This includes:\n\nContextual Analysis: Integrating meta-data with environmental and socioeconomic variables (e.g., location, sampling context) to identify key drivers.\nText Mining & Topic Modeling: Employing Latent Dirichlet Allocation (LDA) on results sections to map the thematic landscape of research trends.\nPredictive Driver Analysis: Applying Extreme Gradient Boosting (XGBoost) with SHAP analysis to identify the most significant features predicting ARG abundance and diversity.\nResistome Community Structure: Using Principal Coordinates Analysis (PCoA) and PERMANOVA to statistically test for differences in ARG composition and abundance between key groups (e.g., plastic vs. water, urbanized vs. non-urbanized).\nContaminant-Driven Selection: Modeling the relationship between antibiotic concentrations (Abx) and ARG abundance using specialized statistical models (e.g., Bregjes model).\n\nThese analyses will form the core of a high-impact manuscript synthesizing the MP-AMR evidence base. Critically, this resource is designed for practical application. For instance, an experimental collaborator such as Dr. Hans-Peter Grossart could use the synthesized findings on DNA extraction protocols and sampling strategies to directly inform and optimize the design of future experimental studies on the plastisphere, ensuring new research is aligned with the existing evidence base.\nLong-Term Vision: A Dynamic Evidence Base for Policy:\nBeyond a static publication, this project aims to create a foundational resource to combat the “big literature” challenge. We intend to launch a publicly accessible, queryable database as a “living review,” designed for periodic updates. This dynamic evidence base will be directly integrated into the wider EU-project TULIP, providing critical data to inform predictive models and interventions against the interconnected threats of plastic pollution, antimicrobial resistance, and climate change."
  }
]